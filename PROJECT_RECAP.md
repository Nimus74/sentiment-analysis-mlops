# ğŸ“š Project Recap - Sentiment Analysis MLOps

**Data Recap**: 8 Febbraio 2026  
**Progetto**: Sistema completo end-to-end di analisi del sentiment con architettura MLOps  
**Linguaggio**: Python 3.10  
**Framework UI**: Gradio (per Hugging Face Spaces) + Streamlit (per monitoring dashboard)  
**API**: FastAPI

---

## ğŸ¯ EXECUTIVE SUMMARY

Sistema MLOps completo per sentiment analysis su testi italiani con due modelli (Transformer RoBERTa e FastText). Include pipeline dati completa (download Hugging Face â†’ preprocessing â†’ split), training con MLflow tracking, API FastAPI per inferenza, monitoring con Evidently AI, e retraining automatico FastText. Deployabile su Hugging Face Spaces (Gradio UI) o Docker (FastAPI). Il progetto Ã¨ funzionante end-to-end con test suite completa e CI/CD GitHub Actions.

---

## ğŸ“ REPO MAP

```
Sentiment_Analisys/
â”œâ”€â”€ app.py                          # ğŸ¯ ENTRY POINT Gradio UI (Hugging Face Spaces)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ main.py                 # ğŸ¯ ENTRY POINT FastAPI service
â”‚   â”‚   â””â”€â”€ schemas.py              # Pydantic schemas per API
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ download_dataset.py     # Download da Hugging Face
â”‚   â”‚   â”œâ”€â”€ preprocessing.py        # Pulizia testi (URL, menzioni, hashtag)
â”‚   â”‚   â”œâ”€â”€ split.py               # Split stratificato train/val/test
â”‚   â”‚   â””â”€â”€ validation.py          # Validazione qualitÃ  dati
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ transformer_model.py   # Wrapper Transformer RoBERTa
â”‚   â”‚   â””â”€â”€ fasttext_model.py      # Wrapper FastText supervised
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ train_transformer.py   # Training/fine-tuning Transformer
â”‚   â”‚   â”œâ”€â”€ train_fasttext.py      # Training FastText
â”‚   â”‚   â”œâ”€â”€ retrain_fasttext.py    # Retraining automatico FastText
â”‚   â”‚   â””â”€â”€ mlflow_utils.py        # Utilities MLflow tracking
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ metrics.py             # Calcolo metriche (macro-F1, etc.)
â”‚   â”‚   â””â”€â”€ compare_models.py      # Confronto modelli
â”‚   â””â”€â”€ monitoring/
â”‚       â”œâ”€â”€ dashboard.py           # ğŸ¯ Streamlit dashboard monitoring
â”‚       â”œâ”€â”€ data_drift.py          # Evidently AI data drift
â”‚       â”œâ”€â”€ data_quality.py        # Evidently AI data quality
â”‚       â”œâ”€â”€ prediction_drift.py    # Evidently AI prediction drift
â”‚       â””â”€â”€ performance_monitoring.py # Performance metrics
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ prepare_data.py            # Script helper preprocessing + split
â”‚   â””â”€â”€ test_api.py                # Script test API
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml                # âš™ï¸ Configurazione centralizzata
â”œâ”€â”€ tests/                         # Test suite pytest
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                       # Dataset scaricati
â”‚   â”œâ”€â”€ processed/                 # Dataset preprocessati + split
â”‚   â””â”€â”€ splits/                     # Indici split salvati
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ transformer/               # Modelli Transformer salvati
â”‚   â””â”€â”€ fasttext/                  # Modelli FastText salvati
â”œâ”€â”€ monitoring/reports/             # Report Evidently AI (HTML)
â”œâ”€â”€ mlruns/                        # MLflow experiment tracking
â”œâ”€â”€ requirements.txt                # âš™ï¸ Dipendenze Python
â”œâ”€â”€ setup.py                       # Package setup
â”œâ”€â”€ Dockerfile                     # âš™ï¸ Container Docker
â”œâ”€â”€ docker-compose.yml             # âš™ï¸ Docker Compose
â””â”€â”€ docs/                          # Documentazione completa

```

**File Chiave**:
- **Entry Points**: `app.py` (Gradio), `src/api/main.py` (FastAPI), `src/monitoring/dashboard.py` (Streamlit)
- **Config**: `configs/config.yaml` (tutto centralizzato)
- **Dependencies**: `requirements.txt`
- **Docker**: `Dockerfile`, `docker-compose.yml`

---

## ğŸ—ï¸ ARCHITETTURA OVERVIEW

### Moduli e ResponsabilitÃ 

#### 1. **Data Pipeline** (`src/data/`)
- **download_dataset.py**: Scarica dataset italiano da Hugging Face (CSV URL diretto), valida formato, calcola hash SHA256 per tracciabilitÃ 
- **preprocessing.py**: Preprocessing deterministico (rimozione URL, menzioni @username, normalizzazione hashtag, caratteri speciali), preparazione formato FastText
- **split.py**: Split stratificato riproducibile 70/15/15 (train/val/test) con seed fisso (42), salvataggio indici per tracciabilitÃ 
- **validation.py**: Genera report qualitÃ  dati (distribuzione classi, lunghezza testi, valori nulli)

#### 2. **Modelli ML** (`src/models/`)
- **transformer_model.py**: Wrapper per `cardiffnlp/twitter-xlm-roberta-base-sentiment` (multilingue, supporta italiano). Supporta fine-tuning, predizione batch, salvataggio/caricamento. Usa pipeline Hugging Face.
- **fasttext_model.py**: Wrapper FastText supervised con workaround NumPy 2.x. Training, predizione batch, salvataggio formato .bin

#### 3. **Training** (`src/training/`)
- **train_transformer.py**: Fine-tuning Transformer con Hugging Face Trainer, early stopping, logging MLflow, salvataggio modello fine-tuned
- **train_fasttext.py**: Training FastText supervised con parametri configurabili, logging MLflow
- **retrain_fasttext.py**: Retraining automatico FastText basato su trigger (data drift, performance degradation, schedule), criteri promozione modello (macro-F1 +2%, max degradazione classe -5%)
- **mlflow_utils.py**: Utilities per setup MLflow, logging parametri/metriche/artefatti, model registry

#### 4. **Evaluation** (`src/evaluation/`)
- **metrics.py**: Calcolo metriche complete (macro-F1 primaria, accuracy, precision, recall per classe, confusion matrix), verifica soglie CI/CD
- **compare_models.py**: Confronto modelli su stesso test set, generazione report comparativo

#### 5. **API Service** (`src/api/`)
- **main.py**: FastAPI app con endpoints `/predict`, `/health`, `/models`, `/feedback`. Lifespan context manager per caricamento modelli all'avvio, cache modelli in memoria, CORS middleware
- **schemas.py**: Pydantic schemas per request/response validation

#### 6. **Monitoring** (`src/monitoring/`)
- **dashboard.py**: Dashboard Streamlit centralizzata per visualizzare report Evidently AI
- **data_drift.py**: Rilevamento data drift con Evidently AI (PSI threshold)
- **data_quality.py**: Report qualitÃ  dati input
- **prediction_drift.py**: Rilevamento drift distribuzione predizioni
- **performance_monitoring.py**: Metriche performance produzione (se ground truth disponibile)

#### 7. **UI Applications**
- **app.py**: Interfaccia Gradio semplice per Hugging Face Spaces, carica modelli all'avvio, predizione con selezione modello (transformer/fasttext)

---

## ğŸ”„ FLOWS END-TO-END

### Flow 1: App Startup (Gradio UI)

```
1. Esegui: python app.py
2. Importa modelli (transformer_model, fasttext_model)
3. load_models() chiamato:
   - Prova a caricare Transformer da models/transformer/final_model/
   - Se non esiste, usa modello pre-addestrato Hugging Face
   - Prova a caricare FastText da models/fasttext/fasttext_model.bin
   - Se non esiste, imposta a None
4. Crea interfaccia Gradio con:
   - Input: Textbox (testo) + Radio (selezione modello)
   - Output: Markdown (risultato formattato)
   - Examples predefiniti
5. demo.launch() â†’ Server Gradio avviato (default: localhost:7860)
```

### Flow 2: User Interaction (Gradio UI)

```
1. Utente inserisce testo nella textbox
2. Utente seleziona modello (transformer/fasttext)
3. Clicca "Submit" o Enter
4. predict_sentiment(text, model_type) chiamato:
   - Valida input (non vuoto)
   - Recupera modello da cache globale
   - Se modello None â†’ ritorna errore
   - model.predict(text) â†’ {label, score, text}
   - Formatta output con emoji e confidence
5. Risultato mostrato in Markdown output
```

### Flow 3: Inference Flow (API FastAPI)

```
1. Client POST /predict con JSON: {"text": "...", "model_type": "transformer"}
2. Pydantic schema valida request (PredictionRequest)
3. Verifica modello disponibile in model_cache
4. Se non disponibile â†’ HTTPException 503
5. model.predict(request.text) eseguito:
   - Transformer: tokenizer â†’ model â†’ softmax â†’ label mapping
   - FastText: model.predict() â†’ rimozione prefisso __label__
6. Risultato formattato in PredictionResponse:
   - text, prediction (label), confidence, model_used, probabilities (opzionale)
7. Ritorna JSON response
```

### Flow 4: Training/Evaluation Flow

```
1. PREPARAZIONE DATI:
   python scripts/prepare_data.py
   â”œâ”€ Carica configs/config.yaml
   â”œâ”€ Download dataset (se non presente):
   â”‚  python -m src.data.download_dataset
   â”‚  â””â”€ Scarica CSV da Hugging Face â†’ data/raw/dataset.csv
   â”œâ”€ Preprocessing:
   â”‚  â””â”€ preprocess_dataframe() â†’ rimozione URL, normalizzazione
   â”œâ”€ Validazione qualitÃ :
   â”‚  â””â”€ generate_quality_report() â†’ data/processed/quality_report.json
   â””â”€ Split stratificato:
      â””â”€ stratified_split() â†’ train.csv, val.csv, test.csv

2. TRAINING FASTTEXT:
   python -m src.training.train_fasttext --config configs/config.yaml
   â”œâ”€ Carica train.csv, val.csv
   â”œâ”€ Prepara formato FastText (__label__<label> <text>)
   â”œâ”€ fasttext.train_supervised() con parametri config
   â”œâ”€ Salva modello â†’ models/fasttext/fasttext_model.bin
   â”œâ”€ Valutazione su val set:
   â”‚  â””â”€ calculate_metrics() â†’ macro-F1, accuracy, etc.
   â””â”€ Logging MLflow:
      â””â”€ log_params(), log_metrics(), log_model_artifact()

3. TRAINING TRANSFORMER:
   python -m src.training.train_transformer --config configs/config.yaml --fine-tune
   â”œâ”€ Carica train.csv, val.csv
   â”œâ”€ Crea SentimentDataset (tokenizzazione)
   â”œâ”€ Carica modello pre-addestrato Hugging Face
   â”œâ”€ Fine-tuning con Trainer:
   â”‚  â”œâ”€ TrainingArguments (epochs, batch_size, learning_rate)
   â”‚  â”œâ”€ EarlyStoppingCallback
   â”‚  â””â”€ compute_metrics callback
   â”œâ”€ Salva modello fine-tuned â†’ models/transformer/final_model/
   â”œâ”€ Valutazione su val set durante training
   â””â”€ Logging MLflow

4. EVALUATION:
   python -m src.evaluation.compare_models
   â”œâ”€ Carica test.csv (holdout set)
   â”œâ”€ Predizioni entrambi modelli su test set
   â”œâ”€ Calcolo metriche per modello
   â”œâ”€ Confronto metriche
   â””â”€ Generazione report â†’ reports/model_comparison/
```

### Flow 5: Monitoring Flow

```
1. GENERAZIONE REPORT (periodica, es. giornaliera):
   python -m src.monitoring.data_drift
   â”œâ”€ Carica reference dataset (train.csv)
   â”œâ”€ Carica current dataset (nuovi dati produzione)
   â”œâ”€ Evidently AI Report con DataDriftPreset
   â”œâ”€ Calcola drift score (PSI)
   â””â”€ Salva report HTML â†’ monitoring/reports/data_drift_report.html

2. DASHBOARD STREAMLIT:
   streamlit run src/monitoring/dashboard.py
   â”œâ”€ Carica ultimi report HTML da monitoring/reports/
   â”œâ”€ Visualizza report Evidently AI embedded
   â”œâ”€ Mostra metriche aggregati
   â””â”€ Interfaccia navigazione tra report

3. RETRAINING TRIGGER:
   python -m src.training.retrain_fasttext
   â”œâ”€ Verifica trigger (data drift, performance, schedule)
   â”œâ”€ Raccoglie nuovi dati da data/feedback.jsonl
   â”œâ”€ Se >= min_samples (100):
   â”‚  â”œâ”€ Combina con training set originale
   â”‚  â”œâ”€ Retrain FastText
   â”‚  â”œâ”€ Valutazione su val set
   â”‚  â”œâ”€ Verifica criteri promozione:
   â”‚  â”‚  â”œâ”€ Macro-F1 miglioramento >= 2%
   â”‚  â”‚  â””â”€ Nessuna classe degradata > 5%
   â”‚  â””â”€ Se promosso: sostituisce modello produzione
   â””â”€ Logging MLflow
```

---

## âœ… CURRENT STATUS

### âœ… IMPLEMENTATO E FUNZIONANTE

1. **Data Pipeline Completa**
   - âœ… Download dataset da Hugging Face
   - âœ… Preprocessing standardizzato
   - âœ… Split stratificato riproducibile
   - âœ… Validazione qualitÃ  dati

2. **Modelli ML**
   - âœ… Transformer RoBERTa (pre-addestrato + fine-tuning)
   - âœ… FastText supervised
   - âœ… Wrapper con interfacce coerenti
   - âœ… Salvataggio/caricamento modelli

3. **Training**
   - âœ… Training Transformer con fine-tuning
   - âœ… Training FastText
   - âœ… MLflow experiment tracking
   - âœ… Early stopping Transformer

4. **Evaluation**
   - âœ… Metriche complete (macro-F1, accuracy, precision, recall)
   - âœ… Confronto modelli
   - âœ… Report generazione

5. **API Service**
   - âœ… FastAPI con endpoints completi
   - âœ… Caricamento modelli all'avvio
   - âœ… Health check
   - âœ… Feedback collection

6. **UI Applications**
   - âœ… Gradio UI per Hugging Face Spaces
   - âœ… Streamlit dashboard monitoring

7. **Monitoring**
   - âœ… Evidently AI integration (data drift, data quality, prediction drift)
   - âœ… Report HTML generazione

8. **Infrastructure**
   - âœ… Docker support
   - âœ… Docker Compose
   - âœ… CI/CD GitHub Actions

9. **Testing**
   - âœ… Test suite pytest completa
   - âœ… Test unitari moduli principali
   - âœ… Test integrazione pipeline

### âš ï¸ PARZIALMENTE IMPLEMENTATO / TODO

1. **Retraining Automatico**
   - âœ… Script retrain_fasttext.py implementato
   - âš ï¸ Trigger automatici non schedulati (richiede setup cron/scheduler esterno)
   - âš ï¸ Retraining Transformer non implementato (solo FastText)

2. **Monitoring Dashboard**
   - âœ… Dashboard Streamlit implementata
   - âš ï¸ Non integrata con sistema di alerting
   - âš ï¸ Report devono essere generati manualmente o via scheduler esterno

3. **API Features**
   - âœ… Endpoint base implementati
   - âš ï¸ Rate limiting non implementato
   - âš ï¸ Authentication non implementata (CORS permissivo)

4. **Documentation**
   - âœ… README presente
   - âœ… Docs/ folder con guide
   - âš ï¸ Alcune discrepanze (menziona Streamlit ma usa Gradio per UI principale)

### âŒ MANCANTE / NON IMPLEMENTATO

1. **Environment Variables**
   - âŒ Nessun file .env.example
   - âŒ Configurazione via variabili d'ambiente non centralizzata

2. **Production Features**
   - âŒ Rate limiting API
   - âŒ Authentication/Authorization
   - âŒ Logging strutturato avanzato (es. JSON logging)
   - âŒ Metrics export per Prometheus/Grafana

3. **Advanced MLOps**
   - âŒ A/B testing modelli in produzione
   - âŒ Feature store
   - âŒ Model versioning avanzato (oltre MLflow)

4. **Deployment Automation**
   - âŒ CI/CD per deploy automatico
   - âŒ Kubernetes manifests (solo Docker)

---

## ğŸš€ HOW TO RUN

### Prerequisiti

- Python 3.10 (richiesto per Evidently AI)
- pip
- Git (opzionale, se clonato da repo)

### Installazione

```bash
# 1. Naviga nella directory progetto
cd /path/to/Sentiment_Analisys

# 2. Crea ambiente virtuale Python 3.10
python3.10 -m venv .venv310
source .venv310/bin/activate  # Windows: .venv310\Scripts\activate

# 3. Installa dipendenze
pip install --upgrade pip
pip install -r requirements.txt

# 4. Installa package in modalitÃ  sviluppo
pip install -e .
```

### Preparazione Dati (Prima Esecuzione)

```bash
# 1. Crea directory necessarie
mkdir -p data/raw data/processed data/splits models/transformer models/fasttext

# 2. Scarica dataset
python -m src.data.download_dataset

# 3. Preprocessing e split
python scripts/prepare_data.py
```

### Training Modelli (Opzionale, se modelli non presenti)

```bash
# Training FastText
python -m src.training.train_fasttext --config configs/config.yaml

# Training Transformer (fine-tuning, richiede tempo)
python -m src.training.train_transformer --config configs/config.yaml --fine-tune
```

### Avvio Applicazioni

#### Opzione 1: Gradio UI (Hugging Face Spaces / Locale)

```bash
# Avvia Gradio UI
python app.py

# Oppure con parametri
python app.py --server_port 7860 --share
```

**Accesso**: http://localhost:7860

#### Opzione 2: FastAPI Service

```bash
# Metodo 1: Direttamente
python -m src.api.main

# Metodo 2: Uvicorn
uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --reload

# Metodo 3: Docker
docker-compose up
```

**Accesso**: http://localhost:8000  
**Docs**: http://localhost:8000/docs

#### Opzione 3: Streamlit Monitoring Dashboard

```bash
streamlit run src/monitoring/dashboard.py
```

**Accesso**: http://localhost:8501

### Test API

```bash
# Health check
curl http://localhost:8000/health

# Predizione
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"text": "Questo prodotto Ã¨ fantastico!", "model_type": "transformer"}'
```

### Esecuzione Test Suite

```bash
# Tutti i test
pytest

# Con coverage
pytest --cov=src --cov-report=html

# Test specifici
pytest tests/test_models.py -v
```

---

## ğŸ“‹ NEXT STEPS (Prioritizzati)

### PrioritÃ  Alta (Per Funzionamento Completo)

1. **Verificare Modelli Presenti**
   - Controllare se `models/transformer/final_model/` e `models/fasttext/fasttext_model.bin` esistono
   - Se mancanti, eseguire training (vedi sezione "Training Modelli")

2. **Verificare Dataset**
   - Controllare se `data/processed/train.csv`, `val.csv`, `test.csv` esistono
   - Se mancanti, eseguire `python scripts/prepare_data.py`

3. **Test End-to-End**
   - Eseguire test suite: `pytest`
   - Testare API: `python scripts/test_api.py`
   - Testare Gradio UI: `python app.py`

### PrioritÃ  Media (Miglioramenti)

1. **Setup Environment Variables**
   - Creare `.env.example` con variabili configurazione
   - Documentare variabili necessarie

2. **Schedulare Monitoring**
   - Setup cron job o scheduler per generazione report Evidently AI periodici
   - Integrare alerting se drift rilevato

3. **Production Hardening**
   - Configurare CORS con origini specifiche (non `["*"]`)
   - Aggiungere rate limiting API
   - Implementare logging strutturato

### PrioritÃ  Bassa (Nice to Have)

1. **Retraining Transformer**
   - Implementare retraining automatico Transformer (attualmente solo FastText)

2. **Advanced Monitoring**
   - Integrazione Prometheus/Grafana
   - Dashboard metriche produzione real-time

3. **Documentation**
   - Allineare README con implementazione reale (Gradio vs Streamlit)
   - Aggiungere diagrammi architettura visuali

---

## â“ QUESTIONS FOR YOU

1. **Modelli Pre-addestrati**: Hai giÃ  modelli addestrati salvati in `models/` o devo eseguire il training da zero?

2. **Dataset**: Il dataset Ã¨ giÃ  scaricato in `data/raw/` o devo scaricarlo?

3. **Deploy Target**: Quale ambiente vuoi usare?
   - Locale (Gradio/FastAPI)
   - Hugging Face Spaces (Gradio)
   - Docker production (FastAPI)
   - Altro?

4. **Monitoring**: Vuoi configurare monitoring automatico con scheduler o Ã¨ sufficiente manuale per ora?

5. **PrioritÃ  Immediate**: Quale componente vuoi testare/eseguire per primo?
   - Gradio UI
   - FastAPI service
   - Training modelli
   - Monitoring dashboard

---

**Fine Recap**
